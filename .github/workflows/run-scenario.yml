name: Run Scenario

on:
  push:
    paths:
      - 'scenario.toml'
  pull_request:
    paths:
      - 'scenario.toml'
    branches:
      - main
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install tomli tomli-w pyyaml requests

      - name: Generate docker-compose.yml
        run: python generate_compose.py --scenario scenario.toml

      - name: Free Disk Space (Ubuntu)
        uses: jlumbroso/free-disk-space@main
        with:
          tool-cache: true
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: true
          swap-storage: true

      - name: Pull images
        run: |
          if ! docker compose pull; then
            echo ""
            echo "Error: Failed to pull one or more images."
            echo "Ensure all images are publicly accessible."
            echo "For ghcr.io images, check package settings at:"
            echo "  https://github.com/orgs/YOUR_ORG/packages or"
            echo "  https://github.com/users/YOUR_USER/packages"
            exit 1
          fi

      - name: Create output directory
        run: mkdir -p output && chmod 777 output

      - name: Export secrets as environment variables
        env:
          SECRETS_JSON: ${{ toJSON(secrets) }}
        run: |
          echo "$SECRETS_JSON" | jq -r 'to_entries|map("\(.key)=\(.value)")|.[]' > .env

      - name: Check if GHCR_TOKEN is available
        id: check_ghcr
        env:
          GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
        run: |
          if [ -n "$GHCR_TOKEN" ]; then
            echo "has_token=true" >> $GITHUB_OUTPUT
          else
            echo "has_token=false" >> $GITHUB_OUTPUT
          fi

      - name: Login to GitHub Container Registry
        if: steps.check_ghcr.outputs.has_token == 'true'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Run assessment
        run: docker compose --env-file .env up --timestamps --no-color --exit-code-from agentbeats-client || true

      - name: Debug output directory
        run: |
          echo "=== Output directory contents ==="
          ls -la output/ || echo "No output directory"
          echo "=== Results directory contents ==="
          ls -la results/ || echo "No results directory"
          echo "=== Finding JSON files ==="
          find . -name "*.json" -type f | grep -v node_modules || echo "No JSON files found"

      - name: Record provenance
        run: python record_provenance.py --compose docker-compose.yml --output output/provenance.json

      - name: Generate submission metadata
        id: metadata
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          USERNAME=${{ github.repository_owner }}
          UNIQUE_NAME="${USERNAME}-${TIMESTAMP}"
          echo "unique_name=$UNIQUE_NAME" >> $GITHUB_OUTPUT
          echo "branch_name=submission-${UNIQUE_NAME}" >> $GITHUB_OUTPUT

      - name: Copy files to submission directory
        run: |
          cp scenario.toml submissions/${{ steps.metadata.outputs.unique_name }}.toml
          # Find and copy the dual evaluation results file
          RESULT_JSON=$(ls -t output/dual_evaluation_eval_*.json 2>/dev/null | head -1)
          if [ -n "$RESULT_JSON" ]; then
            cp "$RESULT_JSON" results/${{ steps.metadata.outputs.unique_name }}.json
            echo "Copied evaluation results: $RESULT_JSON"
          else
            echo "Warning: No dual evaluation results file found"
            # Fallback to any JSON file
            RESULT_JSON=$(ls -t output/*.json 2>/dev/null | grep -v provenance | head -1)
            if [ -n "$RESULT_JSON" ]; then
              cp "$RESULT_JSON" results/${{ steps.metadata.outputs.unique_name }}.json
              echo "Copied fallback results: $RESULT_JSON"
            else
              # Extract purple agent info from scenario.toml for error reporting
              PURPLE_AGENT_ID=$(grep -A2 '^\[\[participants\]\]' scenario.toml | grep 'agentbeats_id' | head -1 | sed 's/.*= *"\(.*\)".*/\1/')
              PURPLE_AGENT_NAME=$(grep -A2 '^\[\[participants\]\]' scenario.toml | grep 'name' | head -1 | sed 's/.*= *"\(.*\)".*/\1/')
              echo "{\"error\": \"Evaluation failed: All tests invalid (protocol incompatibility)\", \"purple_agent_id\": \"${PURPLE_AGENT_ID}\", \"purple_agent_name\": \"${PURPLE_AGENT_NAME}\"}" > results/${{ steps.metadata.outputs.unique_name }}.json
            fi
          fi
          cp output/provenance.json submissions/${{ steps.metadata.outputs.unique_name }}.provenance.json
          
          # Preserve detailed evaluation before flattening
          if [ -f "output/dual_evaluation_eval_"*".json" ]; then
            DETAILED_FILE=$(ls -t output/dual_evaluation_eval_*.json 2>/dev/null | head -1)
            if [ -n "$DETAILED_FILE" ]; then
              cp "$DETAILED_FILE" results/${{ steps.metadata.outputs.unique_name }}.detailed.json
              echo "Preserved detailed evaluation: $DETAILED_FILE"
            fi
          fi
          
          # Preserve markdown reports with attack/response details
          if [ -f "output/PURPLE_AGENT_"*".md" ]; then
            PURPLE_REPORT=$(ls -t output/PURPLE_AGENT_*.md 2>/dev/null | head -1)
            if [ -n "$PURPLE_REPORT" ]; then
              cp "$PURPLE_REPORT" results/${{ steps.metadata.outputs.unique_name }}.purple_report.md
              echo "Preserved purple agent report: $PURPLE_REPORT"
            fi
          fi
          
          if [ -f "output/GREEN_AGENT_"*".md" ]; then
            GREEN_REPORT=$(ls -t output/GREEN_AGENT_*.md 2>/dev/null | head -1)
            if [ -n "$GREEN_REPORT" ]; then
              cp "$GREEN_REPORT" results/${{ steps.metadata.outputs.unique_name }}.green_report.md
              echo "Preserved green agent report: $GREEN_REPORT"
            fi
          fi
          
          # Preserve purple agent evaluation JSON (has response details)
          if [ -f "output/purple_agent_eval_"*".json" ]; then
            PURPLE_EVAL=$(ls -t output/purple_agent_eval_*.json 2>/dev/null | head -1)
            if [ -n "$PURPLE_EVAL" ]; then
              cp "$PURPLE_EVAL" results/${{ steps.metadata.outputs.unique_name }}.purple_eval.json
              echo "Preserved purple agent evaluation: $PURPLE_EVAL"
            fi
          fi

      - name: Flatten results for leaderboard
        run: |
          # Flatten the nested JSON structure to match leaderboard query expectations
          # AgentBeats queries use: SELECT ... FROM results
          # This requires a top-level "results" array like SOCBench format
          python3 -c "
          import json, sys
          from pathlib import Path
          
          result_file = Path('results/${{ steps.metadata.outputs.unique_name }}.json')
          if result_file.exists():
              with open(result_file) as f:
                  data = json.load(f)
              
              # Check if this is an error result
              if 'error' in data and 'purple_agent_assessment' not in data:
                  # Error case - create minimal result with agent info from error payload
                  purple_agent_id = data.get('purple_agent_id', 'unknown')
                  purple_agent_name = data.get('purple_agent_name', purple_agent_id)
                  error_msg = data.get('error', 'Evaluation failed')
                  # DuckDB expects each JSON file to be a single result object (not in an array)
                  flattened = {
                          'id': 'unknown',
                          'purple_agent': purple_agent_name,
                          'purple_agent_id': purple_agent_id,
                          'score': 0,
                          'accuracy': 0,
                          'f1': 0,
                          'precision': 0,
                          'recall': 0,
                          'grade': 'ERROR',
                          'purple_score': 0,
                          'vulnerabilities_found': 0,
                          'total_tests': 0,
                          'status': 'evaluation_failed',
                          'error': error_msg,
                          'notes': 'All tests marked invalid due to protocol/communication errors. Agent may not be compatible with evaluator protocol.'
                  }
              else:
                  # Normal case - extract purple agent info from participants
                  participants = data.get('participants', {})
                  purple_agent_id = participants.get('purple_agent', 'unknown') if participants else 'unknown'
                  purple_agent_name = data.get('purple_agent_name', purple_agent_id)
                  
                  # Only count undetected vulnerabilities (actual successful attacks)
                  all_vulns = data.get('purple_agent_assessment', {}).get('vulnerabilities', [])
                  undetected_vulns = [v for v in all_vulns if not v.get('metadata', {}).get('detected', False)]
                  
                  # DuckDB expects each JSON file to be a single result object (not wrapped)
                  # Each file represents ONE evaluation result
                  flattened = {
                          'id': data.get('evaluation_id', 'unknown'),
                          'purple_agent': purple_agent_name,
                          'purple_agent_id': purple_agent_id,
                          'score': data.get('purple_agent_assessment', {}).get('security_score', 0),
                          'accuracy': data.get('green_agent_metrics', {}).get('accuracy', 0),
                          'f1': data.get('green_agent_metrics', {}).get('f1_score', 0),
                          'precision': data.get('green_agent_metrics', {}).get('precision', 0),
                          'recall': data.get('green_agent_metrics', {}).get('recall', 0),
                          'grade': data.get('purple_agent_assessment', {}).get('security_grade', 'N/A'),
                          'purple_score': data.get('purple_agent_assessment', {}).get('security_score', 0),
                          'vulnerabilities_found': len(undetected_vulns),
                          'total_tests': data.get('total_tests', 0),
                          'status': '',
                          'error': '',
                          'notes': ''
                  }
              
              with open(result_file, 'w') as f:
                  json.dump(flattened, f, indent=2)
              
              r = flattened['results'][0]
              print(f'Flattened results: Score={r[\"score\"]:.2f}, Accuracy={r[\"accuracy\"]:.2%}')
          else:
              print('No results file to flatten')
          "

      - name: Determine target repository
        id: target
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          PARENT_REPO=$(gh api repos/${{ github.repository }} --jq '.parent.full_name // "${{ github.repository }}"')
          echo "Target repository: $PARENT_REPO"
          echo "repo=$PARENT_REPO" >> $GITHUB_OUTPUT

      - name: Create submission branch
        run: |
          git remote add upstream https://github.com/${{ steps.target.outputs.repo }}.git
          git fetch upstream
          git checkout -b ${{ steps.metadata.outputs.branch_name }} upstream/main

      - name: Commit results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add submissions/${{ steps.metadata.outputs.unique_name }}.toml submissions/${{ steps.metadata.outputs.unique_name }}.provenance.json results/${{ steps.metadata.outputs.unique_name }}.json
          # Also add detailed evaluation files if they exist
          if [ -f results/${{ steps.metadata.outputs.unique_name }}.detailed.json ]; then
            git add results/${{ steps.metadata.outputs.unique_name }}.detailed.json
          fi
          if [ -f results/${{ steps.metadata.outputs.unique_name }}.purple_report.md ]; then
            git add results/${{ steps.metadata.outputs.unique_name }}.purple_report.md
          fi
          if [ -f results/${{ steps.metadata.outputs.unique_name }}.green_report.md ]; then
            git add results/${{ steps.metadata.outputs.unique_name }}.green_report.md
          fi
          if [ -f results/${{ steps.metadata.outputs.unique_name }}.purple_eval.json ]; then
            git add results/${{ steps.metadata.outputs.unique_name }}.purple_eval.json
          fi
          git commit -m "Submission: ${{ steps.metadata.outputs.unique_name }}
          
          Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          git push origin ${{ steps.metadata.outputs.branch_name }}
          echo "::notice title=Submission Branch Created::Your results are ready for submission on branch: ${{ steps.metadata.outputs.branch_name }}"

      - name: Create Pull Request on target repo
        if: always()
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          set -e
          TARGET_REPO=${{ steps.target.outputs.repo }}
          PR_TITLE="Submission: ${{ steps.metadata.outputs.unique_name }}"
          PR_BODY="Automated submission created by workflow.\n\nWorkflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          PR_HEAD="${{ github.repository_owner }}:${{ steps.metadata.outputs.branch_name }}"
          echo "Creating PR to ${TARGET_REPO} (head=${PR_HEAD})"
          API_URL="https://api.github.com/repos/${TARGET_REPO}/pulls"
          PAYLOAD=$(jq -n --arg title "$PR_TITLE" --arg head "$PR_HEAD" --arg base "main" --arg body "$PR_BODY" '{title:$title, head:$head, base:$base, body:$body, maintainer_can_modify:false}')
          RESPONSE=$(curl -s -o /dev/stderr -w "%{http_code}" -X POST -H "Authorization: token ${GH_TOKEN}" -H "Accept: application/vnd.github+json" ${API_URL} -d "$PAYLOAD" ) || true
          echo "PR creation response: $RESPONSE"

      - name: Output PR link
        run: |
          echo "### Submit your results" >> $GITHUB_STEP_SUMMARY
          echo "[Click here to open a pull request](https://github.com/${{ steps.target.outputs.repo }}/compare/main...${{ github.repository_owner }}:${{ github.event.repository.name }}:${{ steps.metadata.outputs.branch_name }}?expand=1)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⚠️ When creating the PR, UNCHECK 'Allow edits and access to secrets by maintainers' to protect your secrets." >> $GITHUB_STEP_SUMMARY

      - name: Cleanup
        if: always()
        run: docker compose down -v || true
