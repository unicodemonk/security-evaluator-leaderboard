"""
Vulnerability Manager for tracking and managing discovered vulnerabilities.

Creates and manages vulnerability records from test results.
"""

from typing import List, Optional, Dict, Any
from datetime import datetime

from framework.models import (
    Vulnerability,
    Attack,
    TestResult,
    TestOutcome
)
from framework.scoring.cvss_calculator import CVSSCalculator


class VulnerabilityManager:
    """
    Manages vulnerability creation and tracking.
    
    Converts successful attacks (False Negatives) into vulnerability records
    with CVSS scoring and remediation guidance.
    """
    
    def __init__(self):
        """Initialize vulnerability manager."""
        self.cvss_calculator = CVSSCalculator()
        self.vulnerability_counter = 0
        self.purple_agent_prefix = "PURP"
    
    def create_vulnerability_from_result(
        self,
        attack: Attack,
        result: TestResult,
        purple_agent_name: str
    ) -> Vulnerability:
        """
        Create a vulnerability record from a failed test (False Negative).
        
        Args:
            attack: The attack that succeeded
            result: Test result showing the attack succeeded
            purple_agent_name: Name of the Purple Agent being tested
            
        Returns:
            Vulnerability record with CVSS scoring
        """
        # Generate vulnerability ID
        vuln_id = self._generate_vulnerability_id(purple_agent_name)
        
        # Determine attack type from attack metadata or technique
        attack_type = attack.metadata.get('attack_type', attack.technique)
        category = attack.metadata.get('category', 'Unknown')
        
        # Calculate CVSS score
        cvss_score, cvss_vector = self.cvss_calculator.calculate_from_attack_type(attack_type)
        severity = self.cvss_calculator.get_severity_rating(cvss_score)
        
        # Get CWE information
        cwe_id, cwe_name = self.cvss_calculator.get_cwe_for_attack_type(attack_type)
        
        # Generate description
        description = self._generate_description(attack, attack_type)
        
        # Generate proof of concept
        poc = self._generate_proof_of_concept(attack, result)
        
        # Generate remediation
        remediation = self._generate_remediation(attack_type, attack, result)
        
        # Extract agent response from multiple possible locations
        # 1. Check result.metadata (framework TestResult)
        # 2. Check result.purple_agent_response attribute (SecurityTestResult)
        # 3. Convert PurpleAgentResponse object to dict/string if needed
        agent_response_raw = None
        if hasattr(result, 'metadata') and result.metadata:
            agent_response_raw = (
                result.metadata.get('agent_response') or 
                result.metadata.get('purple_agent_response') or
                result.metadata.get('response')
            )
        
        # Also check if result has purple_agent_response as an attribute (SecurityTestResult)
        if not agent_response_raw and hasattr(result, 'purple_agent_response') and result.purple_agent_response:
            purple_resp = result.purple_agent_response
            # Convert PurpleAgentResponse object to dict
            if hasattr(purple_resp, 'model_dump'):
                agent_response_raw = purple_resp.model_dump()
            elif hasattr(purple_resp, 'dict'):
                agent_response_raw = purple_resp.dict()
            elif isinstance(purple_resp, dict):
                agent_response_raw = purple_resp
            else:
                # Convert to string representation
                agent_response_raw = str(purple_resp)
        
        # Create vulnerability
        vulnerability = Vulnerability(
            vulnerability_id=vuln_id,
            attack_id=attack.attack_id,
            cvss_score=cvss_score,
            severity=severity,
            cwe_id=cwe_id,
            cwe_name=cwe_name,
            description=description,
            proof_of_concept=poc,
            remediation=remediation,
            category=category,
            cvss_vector=cvss_vector,
            metadata={
                'attack_technique': attack.technique,
                'attack_scenario': attack.scenario,
                'timestamp': datetime.now().isoformat(),
                'purple_agent': purple_agent_name,
                # Store payload and response for detailed reporting
                'payload': str(attack.payload),
                'attack_payload': str(attack.payload),  # Alias for compatibility
                # Store response in multiple keys for compatibility
                'agent_response': agent_response_raw,
                'response': agent_response_raw,
                'purple_agent_response': agent_response_raw,
                # MITRE metadata from attack
                'mitre_technique_id': attack.metadata.get('mitre_technique_id'),
                'mitre_technique_name': attack.metadata.get('mitre_technique_name'),
                'mitre_category': attack.metadata.get('category'),
                'mitre_platform': attack.metadata.get('platform'),
                'mitre_severity': attack.metadata.get('severity'),
                'mitre_tactics': attack.metadata.get('mitre_tactics', []),
                'mitre_platforms': attack.metadata.get('mitre_platforms', []),
                'mitre_source': attack.metadata.get('mitre_source'),
                'generation_source': attack.metadata.get('generation_source'),
                # Include test result data (handle both TestResult and SecurityTestResult)
                'detected': getattr(result, 'detected', not getattr(result, 'predicted', False)),
                'confidence': result.confidence,
                'detection_reason': getattr(result, 'detection_reason', None),
                # Include full attack metadata for comprehensive reporting
                **{k: v for k, v in attack.metadata.items() 
                   if k not in ['attack_technique', 'attack_scenario', 'timestamp', 'purple_agent']}
            }
        )
        
        return vulnerability
    
    def create_vulnerabilities_from_results(
        self,
        results: List[TestResult],
        attacks: Dict[str, Attack],
        purple_agent_name: str
    ) -> List[Vulnerability]:
        """
        Create vulnerability records for all successful attacks.
        
        Args:
            results: List of test results
            attacks: Dictionary mapping attack_id to Attack
            purple_agent_name: Name of the Purple Agent
            
        Returns:
            List of vulnerability records
        """
        vulnerabilities = []
        
        for result in results:
            # Check if FALSE_NEGATIVE - handle both Enum types
            is_false_negative = False
            if isinstance(result.outcome, str):
                is_false_negative = 'false' in result.outcome.lower() and 'negative' in result.outcome.lower()
            else:
                is_false_negative = result.outcome == TestOutcome.FALSE_NEGATIVE
            
            # Only create vulnerabilities for successful attacks (False Negatives)
            if is_false_negative:
                # Try both test_case_id and attack_id (different models use different names)
                attack_id = getattr(result, 'attack_id', None) or getattr(result, 'test_case_id', None)
                attack = attacks.get(attack_id)
                if attack and attack.is_malicious:
                    vuln = self.create_vulnerability_from_result(
                        attack, result, purple_agent_name
                    )
                    vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    def _generate_vulnerability_id(self, purple_agent_name: str) -> str:
        """Generate unique vulnerability ID."""
        self.vulnerability_counter += 1
        year = datetime.now().year
        # Use first 4 letters of purple agent name
        agent_prefix = ''.join(c for c in purple_agent_name[:4] if c.isalnum()).upper()
        if not agent_prefix:
            agent_prefix = self.purple_agent_prefix
        return f"{agent_prefix}-{year}-{self.vulnerability_counter:03d}"
    
    def _generate_description(self, attack: Attack, attack_type: str) -> str:
        """Generate vulnerability description."""
        descriptions = {
            "command_injection": (
                "The system accepts and executes arbitrary system commands without "
                "proper validation or sanitization. Attackers can execute any OS "
                "command with the application's privileges."
            ),
            "prompt_injection": (
                "The system's language model can be manipulated through carefully "
                "crafted prompts that override system instructions. Attackers can "
                "bypass safety controls and execute unauthorized actions."
            ),
            "role_confusion": (
                "The system can be tricked into changing its role or ignoring safety "
                "instructions through social engineering in prompts. This allows "
                "bypassing of access controls and security restrictions."
            ),
            "data_exfiltration": (
                "The system can be manipulated to reveal sensitive information "
                "including credentials, private data, or system internals through "
                "carefully crafted requests."
            ),
            "privilege_escalation": (
                "The system allows users to escalate their privileges and access "
                "functionality or data beyond their authorized level through "
                "manipulation of requests or session state."
            ),
            "sql_injection": (
                "The application constructs SQL queries using unsanitized user input, "
                "allowing attackers to inject malicious SQL code and manipulate "
                "database operations."
            ),
            "indirect_prompt_injection": (
                "The system processes external data sources without proper sanitization, "
                "allowing attackers to inject malicious instructions via third-party "
                "content that gets interpreted as system commands."
            ),
        }
        
        default_description = (
            f"The system is vulnerable to {attack_type} attacks. "
            f"Malicious inputs can bypass security controls and cause unauthorized behavior."
        )
        
        return descriptions.get(attack_type.lower(), default_description)
    
    def _generate_proof_of_concept(self, attack: Attack, result: TestResult) -> str:
        """Generate proof of concept with full MITRE details."""
        poc_parts = []
        
        # MITRE Technique Information (if available)
        mitre_technique_id = attack.metadata.get('mitre_technique_id')
        mitre_technique_name = attack.metadata.get('mitre_technique_name')
        
        if mitre_technique_id:
            poc_parts.append(f"MITRE Technique: {mitre_technique_id} - {mitre_technique_name}")
        
        # Attack Details
        poc_parts.append(f"Technique: {attack.technique}")
        poc_parts.append(f"Category: {attack.metadata.get('category', 'N/A')}")
        
        # Attack Payload
        payload_str = str(attack.payload)
        if len(payload_str) > 500:
            poc_parts.append(f"Payload: {payload_str[:500]}... [truncated]")
        else:
            poc_parts.append(f"Payload: {payload_str}")
        
        # Test Result
        poc_parts.append(f"Result: Attack succeeded (not detected/blocked)")
        
        # Handle both TestResult (detected) and SecurityTestResult (predicted)
        detected_status = getattr(result, 'detected', None)
        if detected_status is None:
            detected_status = not getattr(result, 'predicted', False)  # SecurityTestResult: predicted=False means not detected
        
        poc_parts.append(f"Detected: {detected_status}")
        poc_parts.append(f"Confidence: {result.confidence:.2f}")
        
        # Purple Agent Response (if available)
        if hasattr(result, 'metadata') and result.metadata and result.metadata.get('response'):
            response_str = str(result.metadata['response'])
            if len(response_str) > 300:
                poc_parts.append(f"Purple Agent Response: {response_str[:300]}... [truncated]")
            else:
                poc_parts.append(f"Purple Agent Response: {response_str}")
        
        # Additional MITRE metadata
        platform = attack.metadata.get('platform')
        severity = attack.metadata.get('severity')
        if platform:
            poc_parts.append(f"Platform: {platform}")
        if severity:
            poc_parts.append(f"MITRE Severity: {severity}")
        
        return "\n".join(poc_parts)
    
    def _generate_remediation(self, attack_type: str, attack: Attack, result: TestResult) -> str:
        """Generate remediation recommendations."""
        remediations = {
            "command_injection": (
                "1. Disable or remove the system command execution functionality\n"
                "2. Implement a strict allowlist of permitted commands\n"
                "3. Add input validation and sanitization for all command parameters\n"
                "4. Use parameterized command execution with escaped arguments\n"
                "5. Run commands in a sandboxed environment with minimal privileges\n"
                "6. Add logging and monitoring for all command execution attempts"
            ),
            "prompt_injection": (
                "1. Implement system message protection to prevent user override\n"
                "2. Use clear delimiters to separate instructions from user input\n"
                "3. Add output validation before executing any actions\n"
                "4. Implement instruction hierarchy (system > user)\n"
                "5. Use structured input/output formats (JSON schemas)\n"
                "6. Add monitoring for unusual or dangerous response patterns"
            ),
            "role_confusion": (
                "1. Implement strict role-based access control (RBAC)\n"
                "2. Validate all role changes through secure authentication\n"
                "3. Add system message protection against role manipulation\n"
                "4. Use immutable system instructions that cannot be overridden\n"
                "5. Implement session management with role verification\n"
                "6. Add logging for all role change attempts"
            ),
            "data_exfiltration": (
                "1. Implement data classification and access controls\n"
                "2. Remove sensitive data from accessible memory/responses\n"
                "3. Add output filtering to prevent credential exposure\n"
                "4. Use secure credential storage (never in plain text)\n"
                "5. Implement the principle of least privilege\n"
                "6. Add monitoring for unusual data access patterns"
            ),
            "privilege_escalation": (
                "1. Implement proper authentication and authorization checks\n"
                "2. Validate user privileges before every privileged operation\n"
                "3. Use secure session management\n"
                "4. Implement the principle of least privilege\n"
                "5. Add audit logging for privilege changes\n"
                "6. Regular security reviews of access control logic"
            ),
            "sql_injection": (
                "1. Use parameterized queries (prepared statements) exclusively\n"
                "2. Implement input validation and sanitization\n"
                "3. Use an ORM layer with automatic escaping\n"
                "4. Apply the principle of least privilege to database accounts\n"
                "5. Use stored procedures with proper parameter handling\n"
                "6. Add WAF rules to detect and block SQL injection attempts"
            ),
            "indirect_prompt_injection": (
                "1. Sanitize all external data sources before processing\n"
                "2. Use structured data formats with strict schema validation\n"
                "3. Implement context isolation between data and instructions\n"
                "4. Add input validation for all third-party content\n"
                "5. Use content security policies\n"
                "6. Monitor for anomalous behavior from external data sources"
            ),
        }
        
        default_remediation = (
            "1. Implement input validation and sanitization\n"
            "2. Add output validation before executing actions\n"
            "3. Implement proper access controls and authentication\n"
            "4. Use security best practices for the specific attack type\n"
            "5. Add monitoring and logging for security events\n"
            "6. Conduct regular security testing and code reviews"
        )
        
        return remediations.get(attack_type.lower(), default_remediation)
    
    def group_by_severity(self, vulnerabilities: List[Vulnerability]) -> Dict[str, List[Vulnerability]]:
        """
        Group vulnerabilities by severity level.
        
        Args:
            vulnerabilities: List of vulnerabilities
            
        Returns:
            Dictionary mapping severity to list of vulnerabilities
        """
        grouped = {
            "CRITICAL": [],
            "HIGH": [],
            "MEDIUM": [],
            "LOW": [],
            "NONE": []
        }
        
        for vuln in vulnerabilities:
            severity = vuln.severity
            if severity in grouped:
                grouped[severity].append(vuln)
        
        return grouped
    
    def get_top_vulnerabilities(
        self,
        vulnerabilities: List[Vulnerability],
        count: int = 10
    ) -> List[Vulnerability]:
        """
        Get top N vulnerabilities by CVSS score.
        
        Args:
            vulnerabilities: List of vulnerabilities
            count: Number of top vulnerabilities to return
            
        Returns:
            List of top vulnerabilities sorted by CVSS score
        """
        sorted_vulns = sorted(vulnerabilities, key=lambda v: v.cvss_score, reverse=True)
        return sorted_vulns[:count]
