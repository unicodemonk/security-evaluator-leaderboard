"""
Exploiter Agent - Generates attacks near discovered boundaries.

Implements targeted attack generation based on boundary information
from BoundaryProber agents.
"""

from typing import Any, Dict, List, Optional, Set
import random
from datetime import datetime
from pathlib import Path

from ..base import UnifiedAgent, AgentCapabilities, Capability, AgentRole, Task, KnowledgeBase
from ..models import Attack, create_attack_id

# MITRE integration components
try:
    from ..mitre.payload_generator import PayloadGenerator, AttackPayload
    from ..mitre.ttp_selector import MITRETechnique, TTPSource
    MITRE_AVAILABLE = True
except ImportError as e:
    MITRE_AVAILABLE = False
    import logging
    logging.getLogger(__name__).warning(f"MITRE components not available: {e}")


class ExploiterAgent(UnifiedAgent):
    """
    Agent that generates attacks near weak boundaries.

    Uses three sources:
    1. Dataset (40%): Known attacks from datasets
    2. Programmatic (40%): Algorithmic variations
    3. LLM (20%): Creative novel attacks (optional)
    """

    def __init__(
        self,
        agent_id: str,
        knowledge_base: KnowledgeBase,
        scenario: 'SecurityScenario',
        use_llm: bool = False,
        llm_client: Optional[Any] = None,
        model_router: Optional[Any] = None,
        mitre_config: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize exploiter.

        Args:
            agent_id: Unique agent identifier
            knowledge_base: Shared knowledge base
            scenario: Security scenario
            use_llm: Whether to use LLM for creative generation
            llm_client: LLM client (required if use_llm=True)
            model_router: Optional model router for cost optimization
            mitre_config: MITRE configuration (refresh_interval_hours, use_bundled_fallback, etc.)
        """
        capabilities = AgentCapabilities(
            capabilities={Capability.GENERATE},
            role=AgentRole.EXPLOITER,
            requires_llm=use_llm,
            cost_per_invocation=0.05 if use_llm else 0.0,
            avg_latency_ms=1000.0 if use_llm else 50.0
        )
        super().__init__(agent_id, capabilities, knowledge_base)
        self.scenario = scenario
        self.use_llm = use_llm
        self.llm_client = llm_client
        self.model_router = model_router
        self.mitre_config = mitre_config or {}

        # Generation statistics
        self.attacks_generated = 0
        self.generation_sources = {'dataset': 0, 'programmatic': 0, 'llm': 0, 'mitre': 0}
        
        # MITRE integration (optional, graceful fallback)
        self.selected_ttps: List[MITRETechnique] = []
        self._ttps_loaded = False
        
        if MITRE_AVAILABLE and self.mitre_config.get('enabled', True):
            try:
                self.payload_generator = PayloadGenerator(
                    llm_client=llm_client if use_llm else None,
                    use_llm=use_llm,
                    seed=42  # For reproducibility
                )
                self.logger.info("MITRE PayloadGenerator initialized successfully")
            except Exception as e:
                self.logger.warning(f"Failed to initialize MITRE PayloadGenerator: {e}")
                self.payload_generator = None
        else:
            self.payload_generator = None

    def execute_task(self, task: Task) -> Any:
        """
        Execute attack generation task.

        Args:
            task: Task with parameters:
                - technique: Technique to exploit
                - num_attacks: Number of attacks to generate
                - boundary_info: Optional boundary information

        Returns:
            List of generated attacks
        """
        technique = task.parameters.get('technique')
        num_attacks = task.parameters.get('num_attacks', 50)
        boundary_info = task.parameters.get('boundary_info')

        if not technique:
            self.logger.error("Missing required parameter: technique")
            return {'error': 'Missing technique'}

        # MITRE Enhancement: Load TTPs from knowledge base (once)
        if not self._ttps_loaded and self.payload_generator:
            self._load_ttps_from_knowledge_base()

        # Generate attacks (with MITRE enhancement if available)
        attacks = self._generate_attacks(technique, num_attacks, boundary_info)

        # Share knowledge
        self.share_knowledge(
            entry_type='attack',
            data={
                'technique': technique,
                'num_attacks': len(attacks),
                'generation_sources': self.generation_sources,
                'attack_ids': [a.attack_id for a in attacks]
            },
            tags={'attack_generation', technique}
        )

        return {'attacks': attacks, 'count': len(attacks)}

    def _generate_attacks(
        self,
        technique: str,
        num_attacks: int,
        boundary_info: Optional[Dict[str, Any]] = None
    ) -> List[Attack]:
        """
        Generate attacks using hybrid approach.

        Args:
            technique: Attack technique
            num_attacks: Number of attacks to generate
            boundary_info: Optional boundary information for targeting

        Returns:
            List of generated attacks
        """
        attacks = []

        # MITRE Enhancement: Try MITRE-based generation first
        if self.payload_generator and self.selected_ttps:
            mitre_attacks = self._generate_from_mitre(technique, num_attacks)
            if mitre_attacks:
                attacks.extend(mitre_attacks)
                self.generation_sources['mitre'] += len(mitre_attacks)
                self.attacks_generated += len(attacks)
                self.logger.info(f"Generated {len(mitre_attacks)} attacks using MITRE payloads")
                return attacks
            else:
                self.logger.debug(f"No MITRE TTPs found for technique {technique}, using fallback methods")

        # Fallback to original generation methods
        # Split generation: 40% dataset, 40% programmatic, 20% LLM
        num_dataset = int(num_attacks * 0.4)
        num_programmatic = int(num_attacks * 0.4)
        num_llm = num_attacks - num_dataset - num_programmatic if self.use_llm else 0

        # Adjust if no LLM
        if not self.use_llm and num_llm > 0:
            num_programmatic += num_llm
            num_llm = 0

        # 1. Dataset-based generation
        dataset_attacks = self._generate_from_dataset(technique, num_dataset)
        attacks.extend(dataset_attacks)
        self.generation_sources['dataset'] += len(dataset_attacks)

        # 2. Programmatic generation
        programmatic_attacks = self._generate_programmatic(technique, num_programmatic, boundary_info)
        attacks.extend(programmatic_attacks)
        self.generation_sources['programmatic'] += len(programmatic_attacks)

        # 3. LLM generation (if enabled)
        if self.use_llm and num_llm > 0:
            llm_attacks = self._generate_with_llm(technique, num_llm, boundary_info)
            attacks.extend(llm_attacks)
            self.generation_sources['llm'] += len(llm_attacks)

        self.attacks_generated += len(attacks)
        return attacks

    def _generate_from_dataset(self, technique: str, num_attacks: int) -> List[Attack]:
        """
        Generate attacks from baseline dataset.

        Args:
            technique: Attack technique
            num_attacks: Number of attacks

        Returns:
            List of attacks from dataset
        """
        baseline_dataset = self.scenario.get_baseline_dataset()
        if not baseline_dataset:
            return []

        # Filter by technique
        technique_attacks = [a for a in baseline_dataset if a.technique == technique]

        # Sample
        if len(technique_attacks) <= num_attacks:
            return technique_attacks
        else:
            return random.sample(technique_attacks, num_attacks)

    def _generate_programmatic(
        self,
        technique: str,
        num_attacks: int,
        boundary_info: Optional[Dict[str, Any]] = None
    ) -> List[Attack]:
        """
        Generate attacks programmatically.

        Args:
            technique: Attack technique
            num_attacks: Number of attacks
            boundary_info: Optional boundary information

        Returns:
            List of programmatically generated attacks
        """
        attacks = []

        # Get baseline attack
        baseline = self._get_baseline_attack(technique, boundary_info)

        # Generate variations
        for i in range(num_attacks):
            payload = self._create_variation(baseline, i)

            attack = self.scenario.create_attack(
                technique=technique,
                payload=payload,
                metadata={
                    'generation_source': 'programmatic',
                    'variation_index': i,
                    'baseline': baseline
                }
            )
            attack.created_by = self.agent_id
            attacks.append(attack)

        return attacks

    def _generate_with_llm(
        self,
        technique: str,
        num_attacks: int,
        boundary_info: Optional[Dict[str, Any]] = None
    ) -> List[Attack]:
        """
        Generate creative attacks using LLM.

        Args:
            technique: Attack technique
            num_attacks: Number of attacks
            boundary_info: Optional boundary information

        Returns:
            List of LLM-generated attacks
        """
        if not self.llm_client:
            self.logger.warning("LLM client not available, skipping LLM generation")
            return []

        attacks = []

        # Build prompt
        prompt = self._build_generation_prompt(technique, boundary_info)

        try:
            # Route to appropriate model if router available
            if self.model_router:
                task = Task(
                    task_id=f"gen_{technique}_{datetime.now().timestamp()}",
                    task_type='generate_attacks',
                    description=f"Generate {technique} attacks",
                    parameters={'technique': technique, 'num_attacks': num_attacks}
                )
                model_client = self.model_router.route(task, prompt)
                self.logger.info(f"Routed to model: {model_client.name}")
            else:
                model_client = self.llm_client

            # Call LLM
            response = model_client.generate(
                prompt=prompt,
                max_tokens=1000,
                temperature=0.9  # High creativity
            )

            # Parse response into attacks
            payloads = self._parse_llm_response(response)

            for i, payload in enumerate(payloads[:num_attacks]):
                attack = self.scenario.create_attack(
                    technique=technique,
                    payload=payload,
                    metadata={
                        'generation_source': 'llm',
                        'llm_model': model_client.name if hasattr(model_client, 'name') else 'unknown',
                        'prompt_used': prompt
                    }
                )
                attack.created_by = self.agent_id
                attacks.append(attack)

            # Update router with quality feedback
            if self.model_router:
                # Assess quality (simple heuristic: more attacks = better quality)
                quality_score = min(1.0, len(attacks) / num_attacks)
                self.model_router.update('generate_attacks', quality_score)

        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")

        return attacks

    def _get_baseline_attack(self, technique: str, boundary_info: Optional[Dict[str, Any]]) -> str:
        """
        Get baseline attack for variations.

        Args:
            technique: Attack technique
            boundary_info: Optional boundary information

        Returns:
            Baseline attack payload
        """
        # If boundary info provided, use weak boundary payload
        if boundary_info and 'payload' in boundary_info:
            return str(boundary_info['payload'])

        # Otherwise, use technique-specific default
        # (Should be overridden in scenario-specific implementations)
        return f"{technique}_baseline_attack"

    def _create_variation(self, baseline: str, index: int) -> str:
        """
        Create variation of baseline attack.

        Override in scenario-specific implementations.

        Args:
            baseline: Baseline payload
            index: Variation index

        Returns:
            Varied payload
        """
        # Default: Simple variation
        return f"{baseline}_var{index}"

    def _build_generation_prompt(self, technique: str, boundary_info: Optional[Dict[str, Any]]) -> str:
        """
        Build LLM prompt for attack generation.

        Args:
            technique: Attack technique
            boundary_info: Optional boundary information

        Returns:
            LLM prompt
        """
        prompt = f"""Generate creative {technique} attack payloads that may evade detection.

Technique: {technique}
Scenario: {self.scenario.get_name()}

"""

        if boundary_info:
            prompt += f"""Weak boundary detected:
- Payload: {boundary_info.get('payload', 'unknown')}
- Reason: {boundary_info.get('reason', 'unknown')}

Generate variations that exploit this weak boundary.
"""

        prompt += """
Generate 10 attack payloads, one per line.
Focus on novel, creative approaches that differ from common attacks.
"""

        return prompt

    def _parse_llm_response(self, response: str) -> List[str]:
        """
        Parse LLM response into payloads.

        Args:
            response: LLM response text

        Returns:
            List of payloads
        """
        # Simple parsing: One payload per line
        lines = response.strip().split('\n')
        payloads = [line.strip() for line in lines if line.strip()]
        return payloads

    def _load_ttps_from_knowledge_base(self) -> None:
        """
        Load selected MITRE TTPs from knowledge base (shared by BoundaryProber).
        """
        try:
            # Query knowledge base for selected TTPs
            ttp_entries = self.knowledge_base.query(tags={'selected_ttps'})
            
            if ttp_entries:
                # Get the most recent TTP selection (ttp_entries are KnowledgeEntry objects)
                latest_entry = max(ttp_entries, key=lambda e: e.timestamp)
                techniques_data = latest_entry.data.get('techniques', [])
                
                # Convert to MITRETechnique objects
                for ttp_data in techniques_data:
                    technique = MITRETechnique(
                        technique_id=ttp_data['technique_id'],
                        name=ttp_data['name'],
                        description=ttp_data.get('description', ''),
                        tactics=ttp_data.get('tactics', []),
                        platforms=ttp_data.get('platforms', []),
                        data_sources=ttp_data.get('data_sources', []),
                        source=TTPSource(ttp_data.get('source', 'ATTACK'))
                    )
                    self.selected_ttps.append(technique)
                
                self.logger.info(f"Loaded {len(self.selected_ttps)} MITRE TTPs from knowledge base")
            else:
                self.logger.debug("No MITRE TTPs found in knowledge base")
            
            self._ttps_loaded = True
            
        except Exception as e:
            self.logger.warning(f"Failed to load TTPs from knowledge base: {e}")
            self._ttps_loaded = True  # Don't retry

    def _generate_from_mitre(self, technique: str, num_attacks: int) -> List[Attack]:
        """
        Generate attacks using MITRE PayloadGenerator.
        
        Args:
            technique: Attack technique
            num_attacks: Number of attacks to generate
            
        Returns:
            List of Attack objects
        """
        attacks = []
        
        try:
            # Find matching MITRE technique
            # Map scenario technique to MITRE technique
            matching_ttp = self._find_matching_ttp(technique)
            
            if not matching_ttp:
                self.logger.debug(f"No matching MITRE TTP for technique: {technique}")
                return []
            
            # Generate payloads using MITRE PayloadGenerator
            payload_objects = self.payload_generator.generate_payloads_for_technique(
                technique=matching_ttp,
                num_payloads=num_attacks,
                include_benign=True,  # Include benign for FP testing
                platform=self._determine_platform(matching_ttp)
            )
            
            # Convert AttackPayload objects to framework Attack objects
            for payload_obj in payload_objects:
                attack = self.scenario.create_attack(
                    technique=technique,
                    payload=payload_obj.payload,
                    metadata={
                        'mitre_technique_id': payload_obj.technique_id,
                        'mitre_technique_name': payload_obj.technique_name,
                        'generation_method': 'mitre_payload_generator',
                        'generation_source': 'mitre',
                        'severity': payload_obj.severity,
                        'category': payload_obj.category,
                        'platform': payload_obj.platform,
                        'is_malicious': payload_obj.is_malicious
                    }
                )
                attack.created_by = self.agent_id
                attack.is_malicious = payload_obj.is_malicious
                attack.expected_detection = payload_obj.is_malicious
                attacks.append(attack)
            
            self.logger.info(f"Generated {len(attacks)} attacks from MITRE TTP {matching_ttp.technique_id}")
            
        except Exception as e:
            self.logger.error(f"MITRE payload generation failed: {e}")
            return []
        
        return attacks

    def _find_matching_ttp(self, technique: str) -> Optional[MITRETechnique]:
        """
        Find MITRE TTP matching the scenario technique.
        
        Args:
            technique: Scenario technique name
            
        Returns:
            Matching MITRETechnique or None
        """
        # Direct match by name
        technique_lower = technique.lower()
        
        for ttp in self.selected_ttps:
            # Check if technique name matches
            if technique_lower in ttp.name.lower():
                return ttp
            
            # Check tactics
            if technique_lower in [t.lower() for t in ttp.tactics]:
                return ttp
        
        # Fallback: Return first TTP (better than nothing)
        if self.selected_ttps:
            self.logger.debug(f"No exact match for {technique}, using first TTP: {self.selected_ttps[0].technique_id}")
            return self.selected_ttps[0]
        
        return None

    def _determine_platform(self, ttp: MITRETechnique) -> str:
        """
        Determine target platform from TTP.
        
        Args:
            ttp: MITRE technique
            
        Returns:
            Platform string
        """
        if ttp.platforms:
            return ttp.platforms[0]
        return 'generic'

    def can_execute(self, task: Task) -> bool:
        """Check if agent can execute task."""
        return task.task_type in ['generate_attacks', 'exploit']

    def get_generation_stats(self) -> Dict[str, Any]:
        """
        Get generation statistics.

        Returns:
            Dictionary with generation stats
        """
        return {
            'total_generated': self.attacks_generated,
            'sources': self.generation_sources.copy(),
            'source_percentages': {
                k: (v / self.attacks_generated * 100) if self.attacks_generated > 0 else 0
                for k, v in self.generation_sources.items()
            }
        }

